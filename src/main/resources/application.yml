# 数据源配置
spring:
  servlet:
    multipart:
      enabled: true
      max-file-size: 10MB
      max-request-size: 100MB
  application:
    name: enjoy-eat
  profiles:
    active: dev
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    driver-class-name: com.mysql.cj.jdbc.Driver
    #连接URL useUnicode=true&characterEncoding=utf-8
    #1. 存数据时：
    #数据库在存放项目数据的时候会先用UTF-8格式将数据解码成字节码，然后再将解码后的字节码重新使用GBK编码存放到数据库中。
    #2.取数据时：
    #在从数据库中取数据的时候，数据库会先将数据库中的数据按GBK格式解码成字节码，然后再将解码后的字节码重新按UTF-8格式编码数据，最后再将数据返回给客户端
    # zeroDateTimeBehavior=convertToNull: 为0时间不能正确处理 会抛出异常，例如时间。这样就会转换为null
    #allowMultiQueries=true Mybatis批量更新或者批量插入
    #serverTimezone=Asia/Shanghai 修正时间区
    url: jdbc:mysql://192.168.2.2:3306/college?useUnicode=true&characterEncoding=UTF-8&useSSL=false&serverTimezone=Asia/Shanghai
    username: myroot
    password: "1qaz!QAZ"
    # 初始化大小，最小，最大
    initialSize: 1
    minIdle: 4
    #https://blog.csdn.net/w05980598/article/details/78797310/
#    如果有长连接或者费时的链接，可以增加连接数
    maxActive: 8
    # 配置获取连接等待超时的时间
    maxWait: 60000
    # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒
    timeBetweenEvictionRunsMillis: 60000
    # 配置一个连接在池中最小生存的时间，单位是毫秒
    minEvictableIdleTimeMillis: 30000
    validationQuery: select 'x'
    testWhileIdle: true
    testOnBorrow: false
    testOnReturn: false
    # 打开PSCache，并且指定每个连接上PSCache的大小
    poolPreparedStatements: true
    maxPoolPreparedStatementPerConnectionSize: 20
    # 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙
    filters: stat,wall,slf4j
    # 通过connectProperties属性来打开mergeSql功能；慢SQL记录
    connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000
    # 合并多个DruidDataSource的监控数据
    #useGlobalDataSourceStat: true

  #redis配置
  #  参数详解 https://www.cnblogs.com/taiyonghai/p/9454764.html
  redis:
    database: 3
    host: 192.168.2.2
    port: 6379
      #    使用的模式是netty
      #    lettuce:
      #      pool:
      # 连接池最大连接数（使用负值表示没有限制
      #        max-active: 8
      #        max-wait: 10000
      #        max-idle: 8
    #        min-idle: 0
    # 关闭超时时间
  #      shutdown-timeout: 100
  #    password: Hzydkj@75342
  kafka:
    producer:
      bootstrap-servers: 192.168.2.2:9092
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      properties:
#        如果是properties 那么 spring.kafka.producer.properties.request.timeout.ms=10
#        一个请求的相应时间
        request.timeout.ms: 10

    #指定消息被消费之后自动提交偏移量，以便下次继续消费
    consumer:
      enable-auto-commit: true
      group-id: test
      bootstrap-servers: 192.168.2.2:9092
      #指定从最近地方开始消费
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer


mybatis:
  mapperLocations: classpath:college.dao/*.xml
  #开启驼峰命名映射
  configuration:
    map-underscore-to-camel-case: true
    log-impl: org.apache.ibatis.logging.slf4j.Slf4jImpl


# 分页配置
pagehelper:
  reasonable: true
  support-methods-arguments: true
  params: count=countSql


#logger
logging:
  file: ./logs/log
  config: classpath:logback-spring.xml
  level:
    yidao.com.mapper: debug
server:
  port: 1004
#  tomcat:
#    min-spare-threads: 100
#    max-connections: 10000
#    max-threads: 200
